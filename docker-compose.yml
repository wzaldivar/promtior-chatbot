services:

  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 15s
      timeout: 5s
      retries: 10

  model-init:
    image: ollama/ollama:latest
    restart: no
    profiles:
      - init-model
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - LLM_MODEL=${LLM_MODEL:-tinyllama}
    entrypoint: sh -c "ollama pull $${LLM_MODEL}"

  chatbot-ingestor:
    image: wzaldivar/chatbot-ingestor:latest
    build:
      context: .
      dockerfile: chatbot-ingestor.Dockerfile
    restart: no
    profiles:
      - chatbot-ingestion
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./docker_mount/config/:/config/:ro
      - ./docker_mount/data/:/data

  chatbot-server:
    image: wzaldivar/chatbot-server:latest
    build:
      context: .
      dockerfile: chatbot-server.Dockerfile
    restart: unless-stopped
    profiles:
      - chatbot-server
    depends_on:
      ollama:
        condition: service_healthy
    ports:
      - "8000:8000"
    volumes:
      - ./docker_mount/config/:/config/:ro
      - ./docker_mount/data/:/data:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3 


volumes:
  ollama_data:
